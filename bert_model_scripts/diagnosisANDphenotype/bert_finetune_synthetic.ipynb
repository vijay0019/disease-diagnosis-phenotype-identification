{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d9ffd6",
   "metadata": {},
   "source": [
    "# Bio_ClinicalBERT Finetuning(synthetic data) and Llama Accuracy check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b052782",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "    - Read text and annotation from file\n",
    "    - Tokenize text\n",
    "    - Align with annotations\n",
    "    - Generate BIO tags (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43b84210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 104\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def align_tokens_and_labels(text, entities, tokenizer):\n",
    "    \"\"\"\n",
    "    Align tokens with BIO labels.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    tokens = tokenizer.tokenize(text, add_special_tokens=False)\n",
    "    token_spans = tokenized[\"offset_mapping\"]\n",
    "    labels = [\"O\"] * len(token_spans)\n",
    "\n",
    "    for spans, _ in entities:\n",
    "        start, end = spans\n",
    "        is_first = True\n",
    "        for idx, (tok_start, tok_end) in enumerate(token_spans):\n",
    "            if tok_start >= start and tok_end <= end:\n",
    "                if is_first:\n",
    "                    labels[idx] = \"B-HPO\"\n",
    "                    is_first = False\n",
    "                else:\n",
    "                    labels[idx] = \"I-HPO\"\n",
    "\n",
    "    aligned_labels = []\n",
    "    for idx, (token, label) in enumerate(zip(tokens, labels)):\n",
    "        subwords = tokenizer.tokenize(token)\n",
    "        if len(subwords) == 1:\n",
    "            aligned_labels.append(label)\n",
    "        else:\n",
    "            aligned_labels.append(label)\n",
    "            aligned_labels.extend([\"I-HPO\" if label != \"O\" else \"O\"] * (len(subwords) - 1))\n",
    "\n",
    "    return tokens, aligned_labels\n",
    "\n",
    "def preprocess_data(file_path, tokenizer):\n",
    "    data = []\n",
    "\n",
    "    with open('datasets/hpo_dataset.txt', 'r') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    blocks = file_content.strip().split('\\n---\\n')\n",
    "    print(f'total_examples:', len(blocks))\n",
    "        \n",
    "    for block in blocks:\n",
    "        clinical_text_match = re.search(r\"\\*\\*Clinical Text\\*\\*:\\s*(.*?)(?=\\*\\*Annotations\\*\\*:|\\Z)\", block, flags=re.DOTALL)\n",
    "        annotations_match = re.findall(r\"T(\\d+)\\s+LABEL\\s+(\\d+)\\s+(\\d+)\\s+(.*)\", block)\n",
    "        \n",
    "        if clinical_text_match:\n",
    "            clinical_text = clinical_text_match.group(1).strip().split('\"')[1]\n",
    "            \n",
    "            entities = []\n",
    "            for annotation in annotations_match:\n",
    "                associated_text = annotation[3].strip()\n",
    "                matches = re.finditer(re.escape(associated_text), clinical_text)\n",
    "                for _ in matches:\n",
    "                    entities.append(((_.start(), _.end()), _.group()))\n",
    "        \n",
    "        tokens, labels = align_tokens_and_labels(clinical_text, entities, tokenizer)\n",
    "        data.append((tokens, labels))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Preprocess data\n",
    "data = preprocess_data(\"datasets/hpo_dataset.txt\", tokenizer)\n",
    "random.shuffle(data)\n",
    "test_size = 0.1\n",
    "split_idx = int(len(data) * (1 - test_size))\n",
    "train_data = data[:split_idx]\n",
    "test_data = data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf9574",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e3b94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "MAX_LEN = 512\n",
    "\n",
    "class HPODataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = {\"O\": 0, \"B-HPO\": 1, \"I-HPO\": 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, labels = self.data[idx]\n",
    "        encoded = self.tokenizer(tokens,\n",
    "                                is_split_into_words=True,\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True,\n",
    "                                max_length=MAX_LEN,\n",
    "                                return_tensors=\"pt\",\n",
    "                                )\n",
    "\n",
    "        label_ids = [self.label_map[label] for label in labels]\n",
    "        label_ids = label_ids[:MAX_LEN]\n",
    "        label_ids += [0] * (MAX_LEN - len(label_ids))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = HPODataset(train_data, tokenizer)\n",
    "test_dataset = HPODataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed93ddb",
   "metadata": {},
   "source": [
    "### Finetuning Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e843aed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.977300</td>\n",
       "      <td>0.832254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>0.701389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.595237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.587900</td>\n",
       "      <td>0.509175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.502500</td>\n",
       "      <td>0.440618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.435500</td>\n",
       "      <td>0.387476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>0.347988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.320591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.304126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>0.297621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2976209819316864, 'eval_runtime': 0.076, 'eval_samples_per_second': 144.747, 'eval_steps_per_second': 13.159, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_hpo_bert_synthetic3/tokenizer_config.json',\n",
       " './saved_hpo_bert_synthetic3/special_tokens_map.json',\n",
       " './saved_hpo_bert_synthetic3/vocab.txt',\n",
       " './saved_hpo_bert_synthetic3/added_tokens.json')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=len(train_dataset.label_map))\n",
    "for param in model.parameters():\n",
    "    if not param.is_contiguous():\n",
    "        param.data = param.data.contiguous()\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./saved_hpo_bert_synthetic3\",\n",
    "    logging_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=6e-6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)\n",
    "\n",
    "save_path = './saved_hpo_bert_synthetic3'\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbff4f5",
   "metadata": {},
   "source": [
    "### Extraction Code and Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c1c5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24961fa8fb04bd3b0d367a2b2c367ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# best synthetic2\n",
    "bert_model_path = './saved_hpo_bert_synthetic2'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_path)\n",
    "bert_model = AutoModelForTokenClassification.from_pretrained(bert_model_path)\n",
    "bert_model.eval()\n",
    "labels = {0: \"O\", 1: \"B-HPO\", 2:\"I-HPO\"}\n",
    "\n",
    "model_name = '../generative_models/saved_llama_3.2_3B_ins'\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "disclamer = \"Disclamer:\\nThe information provided is for educational purposes and should not replace professional medical advice. Individuals should consult healthcare professionals or local health authorities for personalized guidance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f213edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phenotypes(text):\n",
    "    \"\"\"\n",
    "    Extract HPO terms from text.\n",
    "    \n",
    "    Arg:\n",
    "    - text (str): Input text for NER.\n",
    "    \n",
    "    Returns:\n",
    "    - List of recognized HPO terms.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    pred_labels = predictions[0].tolist()\n",
    "    \n",
    "    recognized_entities = []\n",
    "    current_entity = []\n",
    "    \n",
    "    for token, label_id in zip(tokens, pred_labels):\n",
    "        label = labels.get(label_id, \"O\")\n",
    "\n",
    "        if label == \"B-HPO\":\n",
    "            if current_entity:\n",
    "                recognized_entities.append(\" \".join(current_entity))\n",
    "            current_entity = [token]\n",
    "        elif label == \"I-HPO\":\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                recognized_entities.append(\" \".join(current_entity))\n",
    "                current_entity = []\n",
    "\n",
    "    if current_entity:\n",
    "        recognized_entities.append(\" \".join(current_entity))\n",
    "    \n",
    "    hpo_terms = [\" \".join(e.replace(\" ##\", \"\").replace(\"##\", \"\") for e in entity.split()) for entity in recognized_entities]\n",
    "    \n",
    "    return hpo_terms\n",
    "\n",
    "def get_diagnosis(text, with_bert=False):\n",
    "    \n",
    "    if with_bert:\n",
    "        text = get_phenotypes(text)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert and experienced from the healthcare and biomedical domain with extensive medical knowledge and practical experience. Diagnose the condition based on given text. Also I'm just using it for my project so be consistent every time.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Based on this text give me the disease name only. TEXT: {text}\"},\n",
    "    ]\n",
    "\n",
    "    tokenized_message = llama_tokenizer.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\", return_dict=True)\n",
    "    response_token_ids = llama_model.generate(tokenized_message['input_ids'].cuda(),\n",
    "                                              attention_mask=tokenized_message['attention_mask'].cuda(),\n",
    "                                              max_new_tokens=128, \n",
    "                                              pad_token_id = llama_tokenizer.eos_token_id\n",
    "                                             )\n",
    "    generated_tokens =response_token_ids[:, len(tokenized_message['input_ids'][0]):]\n",
    "    diagnosis = llama_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    return ' '.join(diagnosis.split()[1:])\n",
    "\n",
    "\n",
    "def compare(y, y1, y2, y3):\n",
    "    \n",
    "    yl = len(y)\n",
    "    \n",
    "    if yl <= len(y1):\n",
    "        if y in y1:\n",
    "            return True\n",
    "    else:\n",
    "        if y1 in y:\n",
    "            return True\n",
    "\n",
    "    if yl <= len(y2):\n",
    "        if y in y2:\n",
    "            return True\n",
    "    else:\n",
    "        if y2 in y:\n",
    "            return True\n",
    "        \n",
    "    if yl <= len(y3):\n",
    "        if y in y3:\n",
    "            return True\n",
    "    else:\n",
    "        if y3 in y:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456c808f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’ve been feeling really tired, and I’m notici...</td>\n",
       "      <td>Congestive Heart Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My throat hurts when I swallow, and I have a f...</td>\n",
       "      <td>Streptococcal Pharyngitis (Strep Throat)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have sharp chest pain that gets worse with d...</td>\n",
       "      <td>Pneumothorax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have frequent urination, excessive thirst, a...</td>\n",
       "      <td>Diabetes Mellitus (Type 1 or Type 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My joints hurt and are swollen, especially in ...</td>\n",
       "      <td>Rheumatoid Arthritis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>I feel full after eating only a small amount o...</td>\n",
       "      <td>Gastroparesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>I’ve been losing weight without trying, and I’...</td>\n",
       "      <td>Hyperthyroidism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>I have a constant headache, sensitivity to lig...</td>\n",
       "      <td>Migraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I’m feeling lightheaded, with blurred vision, ...</td>\n",
       "      <td>Hypoglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>I’ve been experiencing excessive thirst and fr...</td>\n",
       "      <td>Diabetes Insipidus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   I’ve been feeling really tired, and I’m notici...   \n",
       "1   My throat hurts when I swallow, and I have a f...   \n",
       "2   I have sharp chest pain that gets worse with d...   \n",
       "3   I have frequent urination, excessive thirst, a...   \n",
       "4   My joints hurt and are swollen, especially in ...   \n",
       "..                                                ...   \n",
       "95  I feel full after eating only a small amount o...   \n",
       "96  I’ve been losing weight without trying, and I’...   \n",
       "97  I have a constant headache, sensitivity to lig...   \n",
       "98  I’m feeling lightheaded, with blurred vision, ...   \n",
       "99  I’ve been experiencing excessive thirst and fr...   \n",
       "\n",
       "                                     disease  \n",
       "0                   Congestive Heart Failure  \n",
       "1   Streptococcal Pharyngitis (Strep Throat)  \n",
       "2                               Pneumothorax  \n",
       "3       Diabetes Mellitus (Type 1 or Type 2)  \n",
       "4                       Rheumatoid Arthritis  \n",
       "..                                       ...  \n",
       "95                             Gastroparesis  \n",
       "96                           Hyperthyroidism  \n",
       "97                                  Migraine  \n",
       "98                              Hypoglycemia  \n",
       "99                        Diabetes Insipidus  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On synthetic testset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/symptom_disease.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b578e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Llama without bert:  0.35\n"
     ]
    }
   ],
   "source": [
    "df['output1'] = df['text'].apply(lambda x: get_diagnosis(x))\n",
    "df['output2'] = df['text'].apply(lambda x: get_diagnosis(x))\n",
    "df['output3'] = df['text'].apply(lambda x: get_diagnosis(x))\n",
    "df['is_correct'] = df.apply(lambda x: compare(x['disease'], x['output1'], x['output2'], x['output3']), axis=1)\n",
    "print(\"Accuracy of Llama without bert: \", (df['is_correct'].value_counts()[True])/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9a8d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Llama bert synthetic:  0.21\n"
     ]
    }
   ],
   "source": [
    "df['b1'] = df['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df['b2'] = df['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df['b3'] = df['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df['is_correct2'] = df.apply(lambda x: compare(x['disease'], x['b1'], x['b2'], x['b3']), axis=1)\n",
    "\n",
    "if True in df['is_correct2'].values:\n",
    "    print(\"Accuracy of Llama bert synthetic: \", (df['is_correct2'].value_counts()[True])/len(df))\n",
    "else:\n",
    "    print(\"Cannot compute accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2256bbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>Acne</td>\n",
       "      <td>I've been noticing a really nasty rash on my s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>Acne</td>\n",
       "      <td>I woke up today to find that I had a major ras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>Acne</td>\n",
       "      <td>Lately I've been experiencing a skin rash with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>Acne</td>\n",
       "      <td>I woke up this morning to find a really nasty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>Arthritis</td>\n",
       "      <td>I've been feeling really weak in my muscles an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>peptic ulcer disease</td>\n",
       "      <td>My bloody stools have caused me to lose iron a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>urinary tract infection</td>\n",
       "      <td>I have pain in my abdomen, and often get fever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>urinary tract infection</td>\n",
       "      <td>My spirits have been incredibly low, and my pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>urinary tract infection</td>\n",
       "      <td>I have a mild temperature and blood in my pee....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>urinary tract infection</td>\n",
       "      <td>My pee looks cloudy and has storng and foul sm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        label  \\\n",
       "568                      Acne   \n",
       "588                      Acne   \n",
       "559                      Acne   \n",
       "571                      Acne   \n",
       "501                 Arthritis   \n",
       "...                       ...   \n",
       "1113     peptic ulcer disease   \n",
       "905   urinary tract infection   \n",
       "939   urinary tract infection   \n",
       "923   urinary tract infection   \n",
       "904   urinary tract infection   \n",
       "\n",
       "                                                   text  \n",
       "568   I've been noticing a really nasty rash on my s...  \n",
       "588   I woke up today to find that I had a major ras...  \n",
       "559   Lately I've been experiencing a skin rash with...  \n",
       "571   I woke up this morning to find a really nasty ...  \n",
       "501   I've been feeling really weak in my muscles an...  \n",
       "...                                                 ...  \n",
       "1113  My bloody stools have caused me to lose iron a...  \n",
       "905   I have pain in my abdomen, and often get fever...  \n",
       "939   My spirits have been incredibly low, and my pe...  \n",
       "923   I have a mild temperature and blood in my pee....  \n",
       "904   My pee looks cloudy and has storng and foul sm...  \n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On symptom2disease dataset\n",
    "import pandas as pd\n",
    "\n",
    "df2 = pd.read_csv('datasets/Symptom2Disease.csv', usecols=['text', 'label'])\n",
    "df2 = df2.groupby('label').sample(n=4)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947d3ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Llama without bert:  0.2916666666666667\n",
      "Accuracy of Llama bert synthetic:  0.13541666666666666\n"
     ]
    }
   ],
   "source": [
    "df2['output1'] = df2['text'].apply(lambda x: get_diagnosis(x))\n",
    "df2['output2'] = df2['text'].apply(lambda x: get_diagnosis(x))\n",
    "df2['output3'] = df2['text'].apply(lambda x: get_diagnosis(x))\n",
    "df2['is_correct'] = df2.apply(lambda x: compare(x['label'], x['output1'], x['output2'], x['output3']), axis=1)\n",
    "print(\"Accuracy of Llama without bert: \", (df2['is_correct'].value_counts()[True])/len(df2))\n",
    "\n",
    "df2['b1'] = df2['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df2['b2'] = df2['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df2['b3'] = df2['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df2['is_correct2'] = df2.apply(lambda x: compare(x['label'], x['b1'], x['b2'], x['b3']), axis=1)\n",
    "\n",
    "if True in df2['is_correct2'].values:\n",
    "    print(\"Accuracy of Llama bert synthetic: \", (df2['is_correct2'].value_counts()[True])/len(df2))\n",
    "else:\n",
    "    print(\"Cannot compute accuracy\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:.conda-unsloth_env]",
   "language": "python",
   "name": "conda-env-.conda-unsloth_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
