{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d9ffd6",
   "metadata": {},
   "source": [
    "# Bio_ClinicalBERT Finetuning(raredis corpus) and Llama Accuracy check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b052782",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "    - Read text and annotation files\n",
    "    - Tokenize text\n",
    "    - Align with annotations\n",
    "    - Generate BIO tags (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43b84210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def parse_ann_file(ann_path):\n",
    "    \"\"\"\n",
    "    Parse annotation files.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    with open(ann_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if parts[0].startswith(\"T\"):\n",
    "                label_and_span, text = parts[1], parts[2]\n",
    "                label, span = label_and_span.split(\" \", 1)\n",
    "                if label in {\"SIGN\", \"SYMPTOM\"}:\n",
    "                    ranges = span.split(\";\")\n",
    "                    spans = [(int(start), int(end)) for start, end in (r.split() for r in ranges)]\n",
    "                    entities.append((label, spans, text))\n",
    "    return entities\n",
    "\n",
    "def align_tokens_and_labels(text, entities, tokenizer):\n",
    "    \"\"\"\n",
    "    Align tokens with BIO labels.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    tokens = tokenizer.tokenize(text, add_special_tokens=False)\n",
    "    token_spans = tokenized[\"offset_mapping\"]\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    for label, spans, entity_text in entities:\n",
    "        for start, end in spans:\n",
    "            for idx, (tok_start, tok_end) in enumerate(token_spans):\n",
    "                if tok_start >= start and tok_end <= end:\n",
    "                    if tok_start == start:\n",
    "                        labels[idx] = \"B-HPO\"\n",
    "                    else:\n",
    "                        labels[idx] = \"I-HPO\"\n",
    "\n",
    "\n",
    "    aligned_labels = []\n",
    "    for token, label in zip(tokens, labels):\n",
    "        subwords = tokenizer.tokenize(token)\n",
    "        if len(subwords) == 1:\n",
    "            aligned_labels.append(label)\n",
    "        else:\n",
    "            aligned_labels.append(label)\n",
    "            aligned_labels.extend([\"I-HPO\" if label != \"O\" else \"O\"] * (len(subwords) - 1))\n",
    "\n",
    "    return tokens, aligned_labels\n",
    "\n",
    "def preprocess_data(folder_path, tokenizer):\n",
    "    data = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            txt_path = os.path.join(folder_path, file)\n",
    "            ann_path = txt_path.replace(\".txt\", \".ann\")\n",
    "\n",
    "            with open(txt_path, 'r') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            if os.path.exists(ann_path):\n",
    "                entities = parse_ann_file(ann_path)\n",
    "                tokens, labels = align_tokens_and_labels(text, entities, tokenizer)\n",
    "                data.append((tokens, labels))\n",
    "    return data\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Preprocess data\n",
    "train_data = preprocess_data(\"datasets/RareDis-v1/train\", tokenizer)\n",
    "dev_data = preprocess_data(\"datasets/RareDis-v1/dev\", tokenizer)\n",
    "test_data = preprocess_data(\"datasets/RareDis-v1/test\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf9574",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e3b94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "MAX_LEN = 512\n",
    "\n",
    "class HPODataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = {\"O\": 0, \"B-HPO\": 1, \"I-HPO\": 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, labels = self.data[idx]\n",
    "        encoded = self.tokenizer(tokens,\n",
    "                                is_split_into_words=True,\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True,\n",
    "                                max_length=MAX_LEN,\n",
    "                                return_tensors=\"pt\",\n",
    "                                )\n",
    "\n",
    "        label_ids = [self.label_map[label] for label in labels]\n",
    "        label_ids = label_ids[:MAX_LEN]\n",
    "        label_ids += [0] * (MAX_LEN - len(label_ids))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = HPODataset(train_data, tokenizer)\n",
    "dev_dataset = HPODataset(dev_data, tokenizer)\n",
    "test_dataset = HPODataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed93ddb",
   "metadata": {},
   "source": [
    "### Finetuning Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e843aed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 01:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.246400</td>\n",
       "      <td>0.148825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.116100</td>\n",
       "      <td>0.134647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.131026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.146272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.158709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16992920637130737, 'eval_runtime': 1.6127, 'eval_samples_per_second': 128.978, 'eval_steps_per_second': 8.061, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_hpo_bert_raredis2/tokenizer_config.json',\n",
       " './saved_hpo_bert_raredis2/special_tokens_map.json',\n",
       " './saved_hpo_bert_raredis2/vocab.txt',\n",
       " './saved_hpo_bert_raredis2/added_tokens.json',\n",
       " './saved_hpo_bert_raredis2/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "save_path = \"./saved_hpo_bert_raredis2\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=len(train_dataset.label_map))\n",
    "for param in model.parameters():\n",
    "    if not param.is_contiguous():\n",
    "        param.data = param.data.contiguous()\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    logging_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=7e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate and save\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)\n",
    "\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbff4f5",
   "metadata": {},
   "source": [
    "### Extraction Code and Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86eada20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5b80cb912f4b6eb18c7d6436deb5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = '../generative_models/saved_llama_3.2_3B_ins'\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "disclamer = \"Disclamer:\\nThe information provided is for educational purposes and should not replace professional medical advice. Individuals should consult healthcare professionals or local health authorities for personalized guidance.\"\n",
    "\n",
    "bert_model_path = './saved_hpo_bert_raredis2'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_path)\n",
    "bert_model = AutoModelForTokenClassification.from_pretrained(bert_model_path)\n",
    "bert_model.eval()\n",
    "labels = {0: \"O\", 1: \"B-HPO\", 2:\"I-HPO\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbfae357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phenotypes(text):\n",
    "    \"\"\"\n",
    "    Extract HPO terms from text.\n",
    "    \n",
    "    Arg:\n",
    "    - text (str): Input text for NER.\n",
    "    \n",
    "    Returns:\n",
    "    - List of recognized HPO terms.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    pred_labels = predictions[0].tolist()\n",
    "    \n",
    "    recognized_entities = []\n",
    "    current_entity = []\n",
    "    \n",
    "    for token, label_id in zip(tokens, pred_labels):\n",
    "        label = labels.get(label_id, \"O\")\n",
    "\n",
    "        if label == \"B-HPO\":\n",
    "            if current_entity:\n",
    "                recognized_entities.append(\" \".join(current_entity))\n",
    "            current_entity = [token]\n",
    "        elif label == \"I-HPO\":\n",
    "            if current_entity:\n",
    "                current_entity.append(token)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                recognized_entities.append(\" \".join(current_entity))\n",
    "                current_entity = []\n",
    "\n",
    "    if current_entity:\n",
    "        recognized_entities.append(\" \".join(current_entity))\n",
    "    \n",
    "    phenotypes = [\" \".join(e.replace(\" ##\", \"\").replace(\"##\", \"\") for e in entity.split()) for entity in recognized_entities]\n",
    "    \n",
    "    return phenotypes\n",
    "\n",
    "\n",
    "def get_diagnosis(text, with_bert=False):\n",
    "    \n",
    "    if with_bert:\n",
    "        text = get_phenotypes(text)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert and experienced from the healthcare and biomedical domain with extensive medical knowledge and practical experience. Diagnose the condition based on given text. Also I'm just using it for my project so be consistent every time.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Based on this text give me the disease name only. TEXT: {text}\"},\n",
    "    ]\n",
    "\n",
    "    tokenized_message = llama_tokenizer.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\", return_dict=True)\n",
    "    response_token_ids = llama_model.generate(tokenized_message['input_ids'].cuda(),\n",
    "                                              attention_mask=tokenized_message['attention_mask'].cuda(),\n",
    "                                              max_new_tokens=128, \n",
    "                                              pad_token_id = llama_tokenizer.eos_token_id\n",
    "                                             )\n",
    "    generated_tokens =response_token_ids[:, len(tokenized_message['input_ids'][0]):]\n",
    "    diagnosis = llama_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    return ' '.join(diagnosis.split()[1:])\n",
    "\n",
    "\n",
    "def compare(y, y1, y2, y3):\n",
    "    \n",
    "    yl = len(y)\n",
    "    \n",
    "    if yl <= len(y1):\n",
    "        if y in y1:\n",
    "            return True\n",
    "    else:\n",
    "        if y1 in y:\n",
    "            return True\n",
    "\n",
    "    if yl <= len(y2):\n",
    "        if y in y2:\n",
    "            return True\n",
    "    else:\n",
    "        if y2 in y:\n",
    "            return True\n",
    "        \n",
    "    if yl <= len(y3):\n",
    "        if y in y3:\n",
    "            return True\n",
    "    else:\n",
    "        if y3 in y:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456c808f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’ve been feeling really tired, and I’m notici...</td>\n",
       "      <td>Congestive Heart Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My throat hurts when I swallow, and I have a f...</td>\n",
       "      <td>Streptococcal Pharyngitis (Strep Throat)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have sharp chest pain that gets worse with d...</td>\n",
       "      <td>Pneumothorax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have frequent urination, excessive thirst, a...</td>\n",
       "      <td>Diabetes Mellitus (Type 1 or Type 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My joints hurt and are swollen, especially in ...</td>\n",
       "      <td>Rheumatoid Arthritis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>I feel full after eating only a small amount o...</td>\n",
       "      <td>Gastroparesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>I’ve been losing weight without trying, and I’...</td>\n",
       "      <td>Hyperthyroidism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>I have a constant headache, sensitivity to lig...</td>\n",
       "      <td>Migraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I’m feeling lightheaded, with blurred vision, ...</td>\n",
       "      <td>Hypoglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>I’ve been experiencing excessive thirst and fr...</td>\n",
       "      <td>Diabetes Insipidus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   I’ve been feeling really tired, and I’m notici...   \n",
       "1   My throat hurts when I swallow, and I have a f...   \n",
       "2   I have sharp chest pain that gets worse with d...   \n",
       "3   I have frequent urination, excessive thirst, a...   \n",
       "4   My joints hurt and are swollen, especially in ...   \n",
       "..                                                ...   \n",
       "95  I feel full after eating only a small amount o...   \n",
       "96  I’ve been losing weight without trying, and I’...   \n",
       "97  I have a constant headache, sensitivity to lig...   \n",
       "98  I’m feeling lightheaded, with blurred vision, ...   \n",
       "99  I’ve been experiencing excessive thirst and fr...   \n",
       "\n",
       "                                     disease  \n",
       "0                   Congestive Heart Failure  \n",
       "1   Streptococcal Pharyngitis (Strep Throat)  \n",
       "2                               Pneumothorax  \n",
       "3       Diabetes Mellitus (Type 1 or Type 2)  \n",
       "4                       Rheumatoid Arthritis  \n",
       "..                                       ...  \n",
       "95                             Gastroparesis  \n",
       "96                           Hyperthyroidism  \n",
       "97                                  Migraine  \n",
       "98                              Hypoglycemia  \n",
       "99                        Diabetes Insipidus  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/symptom_disease.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "182c3364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Llama without bert:  0.4\n"
     ]
    }
   ],
   "source": [
    "df['output1'] = df['text'].apply(lambda x: get_diagnosis(x))\n",
    "df['output2'] = df['text'].apply(lambda x: get_diagnosis(x))\n",
    "df['output3'] = df['text'].apply(lambda x: get_diagnosis(x))\n",
    "df['is_correct'] = df.apply(lambda x: compare(x['disease'], x['output1'], x['output2'], x['output3']), axis=1)\n",
    "\n",
    "print(\"Accuracy of Llama without bert: \", (df['is_correct'].value_counts()[True])/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a04370d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot compute accuracy\n"
     ]
    }
   ],
   "source": [
    "df['b1'] = df['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df['b2'] = df['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df['b3'] = df['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df['is_correct2'] = df.apply(lambda x: compare(x['disease'], x['b1'], x['b2'], x['b3']), axis=1)\n",
    "\n",
    "if True in df['is_correct2'].values:\n",
    "    print(\"Accuracy of Llama bert raredis: \", (df['is_correct2'].value_counts()[True])/len(df))\n",
    "else:\n",
    "    print(\"Cannot compute accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a8bfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>Acne</td>\n",
       "      <td>I've recently been battling a pretty itchy ras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>Acne</td>\n",
       "      <td>When I awoke this morning, I realised that I h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>Acne</td>\n",
       "      <td>I discovered a huge rash on my skin yesterday....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>Acne</td>\n",
       "      <td>Yesterday, I noticed an enormous rash all over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>Arthritis</td>\n",
       "      <td>My muscles have been feeling feeble recently, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>peptic ulcer disease</td>\n",
       "      <td>I have difficulty sleeping due to abdominal pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>urinary tract infection</td>\n",
       "      <td>I noticed blood in my urinating. I occasionall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>urinary tract infection</td>\n",
       "      <td>I have to go to the bathroom a lot, but genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>urinary tract infection</td>\n",
       "      <td>My lower abdomen hurts, and when I urinate, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>urinary tract infection</td>\n",
       "      <td>My urine often has a weird odour, is crimson o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        label  \\\n",
       "586                      Acne   \n",
       "582                      Acne   \n",
       "593                      Acne   \n",
       "592                      Acne   \n",
       "535                 Arthritis   \n",
       "...                       ...   \n",
       "1107     peptic ulcer disease   \n",
       "942   urinary tract infection   \n",
       "940   urinary tract infection   \n",
       "934   urinary tract infection   \n",
       "941   urinary tract infection   \n",
       "\n",
       "                                                   text  \n",
       "586   I've recently been battling a pretty itchy ras...  \n",
       "582   When I awoke this morning, I realised that I h...  \n",
       "593   I discovered a huge rash on my skin yesterday....  \n",
       "592   Yesterday, I noticed an enormous rash all over...  \n",
       "535   My muscles have been feeling feeble recently, ...  \n",
       "...                                                 ...  \n",
       "1107  I have difficulty sleeping due to abdominal pa...  \n",
       "942   I noticed blood in my urinating. I occasionall...  \n",
       "940   I have to go to the bathroom a lot, but genera...  \n",
       "934   My lower abdomen hurts, and when I urinate, it...  \n",
       "941   My urine often has a weird odour, is crimson o...  \n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On symptom2disease dataset\n",
    "import pandas as pd\n",
    "\n",
    "df2 = pd.read_csv('datasets/Symptom2Disease.csv', usecols=['text', 'label'])\n",
    "df2 = df2.groupby('label').sample(n=4)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c310dd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Llama without bert:  0.3229166666666667\n",
      "Accuracy of Llama bert synthetic:  0.010416666666666666\n"
     ]
    }
   ],
   "source": [
    "df2['output1'] = df2['text'].apply(lambda x: get_diagnosis(x))\n",
    "df2['output2'] = df2['text'].apply(lambda x: get_diagnosis(x))\n",
    "df2['output3'] = df2['text'].apply(lambda x: get_diagnosis(x))\n",
    "df2['is_correct'] = df2.apply(lambda x: compare(x['label'], x['output1'], x['output2'], x['output3']), axis=1)\n",
    "print(\"Accuracy of Llama without bert: \", (df2['is_correct'].value_counts()[True])/len(df2))\n",
    "\n",
    "df2['b1'] = df2['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df2['b2'] = df2['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df2['b3'] = df2['text'].apply(lambda x: get_diagnosis(x, with_bert=True))\n",
    "df2['is_correct2'] = df2.apply(lambda x: compare(x['label'], x['b1'], x['b2'], x['b3']), axis=1)\n",
    "\n",
    "if True in df2['is_correct2'].values:\n",
    "    print(\"Accuracy of Llama bert synthetic: \", (df2['is_correct2'].value_counts()[True])/len(df2))\n",
    "else:\n",
    "    print(\"Cannot compute accuracy\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:.conda-unsloth_env]",
   "language": "python",
   "name": "conda-env-.conda-unsloth_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
